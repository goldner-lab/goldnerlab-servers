# goldnerlab

## 2015-02-27: Night of horror

 * Naturally the day that we realize that the backup server has bad disks/power supply,
   /mnt/array decides life is a bit too dull
 * Background:
    * Parsegian array was briefly turned off while mounted this morning while moving goggle
    * While one of the SATA links came back up without a beat, it seems the other did not, causing the
      array to lose two drives
    * Moreover, the link would not come up even after a rescan
	* Tonight I thought I would reboot to reestablish the link and
      allow the array to resync over the weekend.
 * I rebooted goldnerlab
 * Init complained that /mnt/array wasn't found
 * Looked at mdstat, two arrays listed, both degraded
 * Looking deeper revealed that half of the disks belonging to the
   array (`sd[cefg]`) had version 0.9 md superblocks on the block
   device while the other half of the disks (`sd[abdh]1`) had 1.2
   superblocks on a partition
 * Even more strange is that the 0.9 superblocks were last updated in
   2013 and indicated that the volume was a RAID5, clearly these are
   out of date
 * The newer superblocks look more reasonable, indicating RAID6, last
   updated today
 * How the hell could this be?
 * After a great deal of poking around, Kieran noticed that there were
   single-partition partition tables on `sd[cefg]` but there were no corresponding
   block devices for these partitions
 * `partprobe /dev/sd[cefg]` prompted the kernel to recognize the partitions
 * `mdadm --assemble --scan` brought the md array back up successfully
 * Why the hell did this happen?
 * When I brought the old RAID5 array over to RAID6 I suspect the old
   superblocks were never zeroed
 * I still don't know why the kernel needed `partprobe` to find the partition tables
 * Conclusion:
    * I hate md
    * At least it's over
    * The old volume-level md superblocks need to be zeroed once the backup array is up again
    * affected volumes:
        * sdc: UUID=c01c4a64-7221-b35c-9d12-9b646af6b314
        * sde: UUID=c01c4a64-7221-b35c-9d12-9b646af6b314
        * sdf: UUID=c01c4a64-7221-b35c-9d12-9b646af6b314
        * sdg: UUID=c01c4a64-7221-b35c-9d12-9b646af6b314
 
## 2015-01-19

 * Noted 1 uncorrectable sector,

        Device: /dev/sdo [SAT], 1 Offline uncorrectable sectors

        Device info:
        ST31000528AS, S/N:9VP408G5, WWN:5-000c50-01a809c24, FW:CC38, 1.00 TB

 * Finally have backup working reliably again after `goldner-backup` went down


## 2014-09-10

 * Brought up parsegian array on sil1312 adapter.
 * See [the Parsegian array log](parsegian-array.mkd) for details.

 * Finally going to fix permissions issues (backup permissions and `~lab` issues)
   by enabling POSIX ACLs on `/mnt/array` and `/mnt/parsegian`
 * Gave `bup` user read on `/mnt/array`

     $ sudo setfacl -R -m u:bup:rX /mnt/array

 * Set default ACL for `/home/lab`

     $ sudo setfacl -R -m d:g:lab:rX /home/lab

## 2014-09-03

 * Finally dealt with MoinMoin issues
 * 91000 spam accounts have been created
 * Examined the creation IPs, deleted all accounts created by IPs with
   five or more associated accounts

## 2014-08-22

 * After running out or nearly running out of disk space on  `/` several times,
   I purchased a new drive (Seagate ST2000DM001-1ER164)
 * Tonight I,
    1. booted from installation media
    2. dd'd the old `/` volume to the new drive
    3. deleted the swap partition and old LVM partition from the new drive
    4. did some disk acrobatics to move the old `/srv` partition on
       the LVM volume to `/`
 * While doing this I noticed that both ST31000528AS drives have bad
   sectors:
    * 9VP3YFPT has 520 bad sectors
    * 9VP408G5 has 55 bad sectors
    * all other drives are perfectly healthy
 * Now since the LVM volume is officially unneeded the two failing
   drives aren't terribly important
 * May repurpose them for temporary scratch space at some point in the future
 * In other news, I added a [write-intent bitmap][] to `/mnt/array` with,

        $ mdadm --grow --bitmap=internal /dev/md0

   This should speed rebuilds in the future.
 * Also noticed that WD20EZRX-00D had fallen out of `/dev/md0` as spare
 * Reformatted drive with GPT partition table and re-added to array

[write-intent bitmap]: https://raid.wiki.kernel.org/index.php/Write-intent_bitmap

## 2014-05-30

 * Noted presence of suspicious messages from `smartd` in `/var/log/syslog`. e.g.,

   ```
   smartd[3248]: Device: /dev/sdf, type changed from 'scsi' to 'sat'
   smartd[3248]: Device: /dev/sdf [SAT], opened
   smartd[3248]: Device: /dev/sdf [SAT], SAMSUNG HD204UI, S/N:S2H7JD2ZB07815, WWN:5-0024e9-00450603c, FW:1AQ10001, 2.00 TB
   smartd[3248]: Device: /dev/sdf [SAT], found in smartd database: SAMSUNG SpinPoint F4 EG (AF)
   smartd[3248]: Device: /dev/sdf [SAT], WARNING: Using smartmontools or hdparm with this
   smartd[3248]: drive may result in data loss due to a firmware bug.
   smartd[3248]: ****** THIS DRIVE MAY OR MAY NOT BE AFFECTED! ******
   smartd[3248]: Buggy and fixed firmware report same version number!
   smartd[3248]: See the following web pages for details:
   smartd[3248]: http://knowledge.seagate.com/articles/en_US/FAQ/223571en
   smartd[3248]: http://sourceforge.net/apps/trac/smartmontools/wiki/SamsungF4EGBadBlocks
   smartd[3248]: Device: /dev/sdf [SAT], is SMART capable. Adding to "monitor" list.
   smartd[3248]: Device: /dev/sdf [SAT], state read from /var/lib/smartmontools/smartd.SAMSUNG_HD204UI-S2H7JD2ZB07815.ata.state
   ```

 * Drive affected by [bug](http://sourceforge.net/apps/trac/smartmontools/wiki/SamsungF4EGBadBlocks)
 * Affected drives in goldnerlab RAID array,
     * Device: /dev/sda [SAT], SAMSUNG HD204UI, S/N:S2H7JD2ZB07509, WWN:5-0024e9-004504475, FW:1AQ10001, 2.00 TB
     * Device: /dev/sdb [SAT], SAMSUNG HD204UI, S/N:S2H7J1BZB14648, WWN:5-0024e9-0045279bb, FW:1AQ10001, 2.00 TB
     * Device: /dev/sdc [SAT], SAMSUNG HD204UI, S/N:S2H7JD2ZB07805, WWN:5-0024e9-004505fef, FW:1AQ10001, 2.00 TB
     * Device: /dev/sdd [SAT], SAMSUNG HD204UI, S/N:S2H7J1BZB14653, WWN:5-0024e9-004527b70, FW:1AQ10001, 2.00 TB
     * Device: /dev/sde [SAT], SAMSUNG HD204UI, S/N:S2H7JD2ZB07520, WWN:5-0024e9-0045044cb, FW:1AQ10001, 2.00 TB
     * Device: /dev/sdf [SAT], SAMSUNG HD204UI, S/N:S2H7JD2ZB07815, WWN:5-0024e9-00450603c, FW:1AQ10001, 2.00 TB
     * Device: /dev/sdg [SAT], SAMSUNG HD204UI, S/N:S2H7JD2ZB07511, WWN:5-0024e9-00450448a, FW:1AQ10001, 2.00 TB
     * Device: /dev/sdh [SAT], SAMSUNG HD204UI, S/N:S2H7JD2ZB07806, WWN:5-0024e9-004505ff5, FW:1AQ10001, 2.00 TB
 * Upgraded all affected drives as described in
   `http://knowledge.seagate.com/articles/en_US/FAQ/223571en` using
   FreeDOS image


## 2014-05-08
 * At behest of John Denker `smartd` was enabled
   (`/etc/default/smartmontools`), root's mail forwarded to bgamari (`/root/.forward`)

## 2014-04-11

 * In light of Heartbleed vulnerability regenerated SSL keys as per http://myrddin.org/howto/debian-apache-ssl/
 
## 2013-10-28

 * Noticed media failure on ata4.00

## 2013-10-25

 * Finally lost faith in file system
 * See email to goldnerlab@physics list <id:87wql1dxn9.fsf@gmail.com>
 * Rebuilt array from scratch with new filesystem moving data temporarily to `goldner-backup`

## 2013-10-22
 * Expanded `/dev/md0` filesystem to full size of volume
 * Tried `bup` backup again
 * Same sort of errors as seen on 2013-10-13, resulting in machine dying
 * Noticed quite some memory errors in `/var/log/mcelog` from CPU 1
   bank 8. These look like,

        Fallback Socket memory error count 32753 exceeded threshold: 32753 in 24h
        Location SOCKET:1 CHANNEL:? DIMM:? []
        Running trigger `socket-memory-error-trigger'
        Hardware event. This is not a software error.
        MCE 0
        CPU 1 BANK 8 
        MISC 0 
        TIME 1370564912 Thu Jun  6 20:28:32 2013
        MCG status:
        MCi status:
        Error overflow
        Uncorrected error
        MCi_MISC register valid
        Processor context corrupt
        MCA: MEMORY CONTROLLER AC_CHANNEL0_ERR
        Transaction: Address/Command error
        Memory address parity error
        Memory corrected error count (CORE_ERR_CNT): 32754
        Memory transaction Tracker ID (RTId): 0
        Memory DIMM ID of error: 0
        Memory channel ID of error: 0
        Memory ECC syndrome: 0
        STATUS ea1ffc80008000b0 MCGSTATUS 0
        MCGCAP 1c09 APICID 10 SOCKETID 1 
        CPUID Vendor Intel Family 6 Model 26

 * Really should run `memcheck86` to verify stability
 * Changed SATA cables again, reseated all drives
 * `fsck` on array is taking a very long time, finding many files with
   multiply claimed blocks,

        /backup/home/bgamari/legos/*
        /backup/home/rich/ALEX/09_05_2011/2011-09-05-run_005.timetag
        /backup/home/rich/ALEX/10_25_2011/2011-10-25-run_003.timetag
        /backup/home/rich/ALEX/10_25_2011/2011-10-25-run_006.timetag
        /backup/home/rich/ALEX/10_26_2011/2011-10-26-run_000.timetag
        /backup/home/rich/ALEX/10_26_2011/2011-10-26-run_002.timetag
        /backup/home/rich/ALEX/10_26_2011/2011-10-26-run_004.timetag
        /backup/home/rich/ALEX/10_26_2011/2011-10-26-run_006.timetag
        /backup/home/rich/ALEX/10_26_2011/2011-10-26-run_007.timetag
        /backup/home/rich/ALEX/10_26_2011/2011-10-26-run_008.timetag
        /backup/home/rich/ALEX/10_26_2011/2011-10-26-run_010.timetag
        /backup/home/rich/ALEX/10_26_2011/2011-10-26-run_011.timetag
        /backup/home/rich/ALEX/10_26_2011/2011-10-26-run_012.timetag
        /backup/home/rich/ALEX/10_28_2011/2011-10-28-run_000.timetag
        /backup/home/peker/openmpi-1.2.8/orte/mca/pls/proxy/Makefile.in
        /backup/home/peker/openmpi-1.2.8/orte/mca/pls/proxy/pls_proxy.c
        /backup/home/peker/openmpi-1.2.8/orte/mca/pls/proxy/pls_proxy.h
        /backup/home/peker/openmpi-1.2.8/orte/mca/pls/proxy/pls_proxy_component.c
        /backup/home/peker/openmpi-1.2.8/orte/mca/pls/rsh/Makefile.am
        /backup/home/bgamari/repos/bayes-stack/network-topic-models/sweeps/00170.state

 * Thankfully these are all in the `/backup` directory which is redundant
 * I'll rsync against the backup on `goldner-backup` just to make sure nothing was damaged
 * These blocks are generally all shared with files in `/home/bgamari/.bup/index-cache`
 * Going to move `BUPDIR` to `/` for next backup
 
## 2013-10-19

 * Moved `/dev/md0` from RAID10 to RAID0 to RAID5

        $ mdadm --grow /dev/md0 --backup-file=/var/md-backup --level=0
        $ mdadm --grow /dev/md0 --backup-file=/var/md-backup --level=5

 * Need to expand filesystem after things have synchronized

## 2013-10-13

 * Setting up bup backup of `/mnt/array` to `goldner-backup`
 * Strange slowness of `bup bloom`
 * Still seeing occassional funkiness of `ata2.01`,

   ```
   [2649467.141902] ata2.01: exception Emask 0x0 SAct 0x3f4a952 SErr 0x400000 action 0x6
   [2649467.217305] ata2.01: irq_stat 0x00060002, device error via SDB FIS
   [2649467.293452] ata2.01: SError: { Handshk }
   [2649467.369193] ata2.01: failed command: WRITE FPDMA QUEUED
   [2649467.444859] ata2.01: cmd 61/00:08:3f:b4:f2/04:00:e6:00:00/40 tag 1 ncq 524288 out
   [2649467.444859]          res f4/b2:01:01:00:00/00:00:00:10:f4/00 Emask 0x2 (HSM violation)
   [2649467.598529] ata2.01: status: { Busy }
   [2649467.675040] ata2.01: error: { ICRC IDNF }
   [2649467.750713] ata2.01: failed command: WRITE FPDMA QUEUED
   [2649467.826947] ata2.01: cmd 61/00:20:3f:f8:f2/04:00:e6:00:00/40 tag 4 ncq 524288 out
   [2649467.826947]          res f4/b2:01:01:00:00/00:00:00:40:f4/00 Emask 0x2 (HSM violation)
   [2649467.980702] ata2.01: status: { Busy }
   [2649468.057229] ata2.01: error: { ICRC IDNF }
   [2649468.133538] ata2.01: failed command: WRITE FPDMA QUEUED
   [2649468.209410] ata2.01: cmd 61/00:30:3f:fc:f2/04:00:e6:00:00/40 tag 6 ncq 524288 out
   [2649468.209410]          res f4/b2:01:01:00:00/00:00:00:60:f4/00 Emask 0x2 (HSM violation)
   [2649468.363981] ata2.01: status: { Busy }
   [2649468.441165] ata2.01: error: { ICRC IDNF }
   [2649468.517062] ata2.01: failed command: WRITE FPDMA QUEUED
   [2649468.593622] ata2.01: cmd 61/00:40:3f:f4:f2/04:00:e6:00:00/40 tag 8 ncq 524288 out
   [2649468.593622]          res f4/b2:01:01:00:00/00:00:00:80:f4/00 Emask 0x2 (HSM violation)
   [2649468.748634] ata2.01: status: { Busy }
   [2649468.825379] ata2.01: error: { ICRC IDNF }
   [2649468.901975] ata2.01: failed command: WRITE FPDMA QUEUED
   [2649468.978331] ata2.01: cmd 61/00:58:3f:64:f2/04:00:e6:00:00/40 tag 11 ncq 524288 out
   [2649468.978331]          res f4/b2:01:01:00:00/00:00:00:b0:f4/00 Emask 0x2 (HSM violation)
   [2649469.133575] ata2.01: status: { Busy }
   [2649469.210987] ata2.01: error: { ICRC IDNF }
   [2649469.287550] ata2.01: failed command: WRITE FPDMA QUEUED
   [2649469.364029] ata2.01: cmd 61/00:68:3f:f0:f2/04:00:e6:00:00/40 tag 13 ncq 524288 out
   [2649469.364029]          res f4/b2:01:01:00:00/00:00:00:d0:f4/00 Emask 0x2 (HSM violation)
   [2649469.518738] ata2.01: status: { Busy }
   [2649469.596631] ata2.01: error: { ICRC IDNF }
   [2649469.673598] ata2.01: failed command: WRITE FPDMA QUEUED
   [2649469.751000] ata2.01: cmd 61/00:78:3f:6c:f2/04:00:e6:00:00/40 tag 15 ncq 524288 out
   [2649469.751000]          res 61/00:78:3f:6c:f2/00:00:e6:00:00/40 Emask 0x1 (device error)
   [2649469.906991] ata2.01: status: { DRDY DF ERR }
   ```


## 2013-08-18

 * Due to [bug][collectd-bug] in version of `collectd` distributed in Raring, forced to install from source.
   Source tree can be found in `~bgamari/repos/collectd`, installed to `/opt/collectd`
 * Configured `goggle` to push samples to `goldnerlab`
 * Also finally fixed [visage][] installation on `goldnerlab`, start script can be found at `~bgamari/start-visage.conf`

## 2013-07-17

 * Installed and configured [collectd][] and [visage][] on goldnerlab and goggle
 * Wrote a small [shim][collectd-cuda] to get CUDA statistics into collectd

[collectd]: http://www.collectd.org/
[visage]: https://collectd.org/wiki/index.php/Visage
[collectd-cuda]: https://github.com/bgamari/cuda-collectd

## 2013-06-07
 
 * Noticed that `sdl` dropped out

   ```
   $ dmesg
   [ 2289.401292] end_request: I/O error, dev sdl, sector 223426111
   [ 2289.401314] sd 7:3:0:0: [sdl]  Result: hostbyte=DID_OK driverbyte=DRIVER_SENSE
   [ 2289.401317] sd 7:3:0:0: [sdl]  Sense Key : Aborted Command [current] [descriptor]
   [ 2289.401320] Descriptor sense data with sense descriptors (in hex):
   [ 2289.401322]         72 0b 00 00 00 00 00 0c 00 0a 80 00 00 00 51 e0 
   [ 2289.401329]         00 00 00 01 
   [ 2289.401333] sd 7:3:0:0: [sdl]  Add. Sense: No additional sense information
   [ 2289.401336] sd 7:3:0:0: [sdl] CDB: Write(10): 2a 00 0d 51 3a 3f 00 04 00 00
   [ 2289.401342] end_request: I/O error, dev sdl, sector 223427135
   [ 2289.401359] ata8: EH complete
   [ 2289.401830] ata8.03: detaching (SCSI 7:3:0:0)
   ```

   * Not entiredly sure which drive this is, going to label all drives

 * Looking back at `sdb` again:
   * On for 3.3 years
   * 458 bad sectors

 * Recompiled Amber 11 against CUDA 5.0
 * Upgraded to Ubuntu 12.10 and then 13.04, rebuilt Amber

## 2013-06-06

 * Amber 11 and 12 crashing with a variety of unspecified kernel launch
   failures
 * Moved from CUDA toolkit 4.2 to 5.0
 * Moving from `nvidia` driver version 319.23 to 4.304.54 appears to fix crashes
   in Amber 12
   
## 2013-04-15 

 * `sdb` crashed
   * ST31000528AS (Serial #9VP3YFPT)
 * Removed final data from old `lvm` volumes

## 2011-11-13

 * Power supply failure

## Some time around 2011-09

 * Loss of btrfs data volume due to loose eSATA cable
 * Reformatted volume with more traditional `md` RAID and ext4

## 2011-08-16

 * Upgraded to Ubuntu 11.04

## 2011-01-02

 * Configuration of external 16 TB RAID array

## 2010-02-16

 * Ordered two Seagate Barracuda 7200.12 1TB drivers and new 700W PSU
 * Installed internally

## 2009-12

 * Initial configuration running Ubuntu under VM and other craziness
 * Does anyone remember when exactly this was?

