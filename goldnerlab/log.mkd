## 2013-10-28

 * Noticed media failure on ata4.00

## 2013-10-25

 * Finally lost faith in file system
 * See email to goldnerlab@physics list <id:87wql1dxn9.fsf@gmail.com>
 * Rebuilt array from scratch with new filesystem moving data temporarily to `goldner-backup`

## 2013-10-22
 * Expanded `/dev/md0` filesystem to full size of volume
 * Tried `bup` backup again
 * Same sort of errors as seen on 2013-10-13, resulting in machine dying
 * Noticed quite some memory errors in `/var/log/mcelog` from CPU 1
   bank 8. These look like,

        Fallback Socket memory error count 32753 exceeded threshold: 32753 in 24h
        Location SOCKET:1 CHANNEL:? DIMM:? []
        Running trigger `socket-memory-error-trigger'
        Hardware event. This is not a software error.
        MCE 0
        CPU 1 BANK 8 
        MISC 0 
        TIME 1370564912 Thu Jun  6 20:28:32 2013
        MCG status:
        MCi status:
        Error overflow
        Uncorrected error
        MCi_MISC register valid
        Processor context corrupt
        MCA: MEMORY CONTROLLER AC_CHANNEL0_ERR
        Transaction: Address/Command error
        Memory address parity error
        Memory corrected error count (CORE_ERR_CNT): 32754
        Memory transaction Tracker ID (RTId): 0
        Memory DIMM ID of error: 0
        Memory channel ID of error: 0
        Memory ECC syndrome: 0
        STATUS ea1ffc80008000b0 MCGSTATUS 0
        MCGCAP 1c09 APICID 10 SOCKETID 1 
        CPUID Vendor Intel Family 6 Model 26

 * Really should run `memcheck86` to verify stability
 * Changed SATA cables again, reseated all drives
 * `fsck` on array is taking a very long time, finding many files with
   multiply claimed blocks,

        /backup/home/bgamari/legos/*
        /backup/home/rich/ALEX/09_05_2011/2011-09-05-run_005.timetag
        /backup/home/rich/ALEX/10_25_2011/2011-10-25-run_003.timetag
        /backup/home/rich/ALEX/10_25_2011/2011-10-25-run_006.timetag
        /backup/home/rich/ALEX/10_26_2011/2011-10-26-run_000.timetag
        /backup/home/rich/ALEX/10_26_2011/2011-10-26-run_002.timetag
        /backup/home/rich/ALEX/10_26_2011/2011-10-26-run_004.timetag
        /backup/home/rich/ALEX/10_26_2011/2011-10-26-run_006.timetag
        /backup/home/rich/ALEX/10_26_2011/2011-10-26-run_007.timetag
        /backup/home/rich/ALEX/10_26_2011/2011-10-26-run_008.timetag
        /backup/home/rich/ALEX/10_26_2011/2011-10-26-run_010.timetag
        /backup/home/rich/ALEX/10_26_2011/2011-10-26-run_011.timetag
        /backup/home/rich/ALEX/10_26_2011/2011-10-26-run_012.timetag
        /backup/home/rich/ALEX/10_28_2011/2011-10-28-run_000.timetag
        /backup/home/peker/openmpi-1.2.8/orte/mca/pls/proxy/Makefile.in
        /backup/home/peker/openmpi-1.2.8/orte/mca/pls/proxy/pls_proxy.c
        /backup/home/peker/openmpi-1.2.8/orte/mca/pls/proxy/pls_proxy.h
        /backup/home/peker/openmpi-1.2.8/orte/mca/pls/proxy/pls_proxy_component.c
        /backup/home/peker/openmpi-1.2.8/orte/mca/pls/rsh/Makefile.am
        /backup/home/bgamari/repos/bayes-stack/network-topic-models/sweeps/00170.state

 * Thankfully these are all in the `/backup` directory which is redundant
 * I'll rsync against the backup on `goldner-backup` just to make sure nothing was damaged
 * These blocks are generally all shared with files in `/home/bgamari/.bup/index-cache`
 * Going to move `BUPDIR` to `/` for next backup
 * 
 
## 2013-10-19

 * Moved `/dev/md0` from RAID10 to RAID0 to RAID5

        $ mdadm --grow /dev/md0 --backup-file=/var/md-backup --level=0
        $ mdadm --grow /dev/md0 --backup-file=/var/md-backup --level=5

 * Need to expand filesystem after things have synchronized

## 2013-10-13

 * Setting up bup backup of `/mnt/array` to `goldner-backup`
 * Strange slowness of `bup bloom`
 * Still seeing occassional funkiness of `ata2.01`,

      [2649467.141902] ata2.01: exception Emask 0x0 SAct 0x3f4a952 SErr 0x400000 action 0x6
      [2649467.217305] ata2.01: irq_stat 0x00060002, device error via SDB FIS
      [2649467.293452] ata2.01: SError: { Handshk }
      [2649467.369193] ata2.01: failed command: WRITE FPDMA QUEUED
      [2649467.444859] ata2.01: cmd 61/00:08:3f:b4:f2/04:00:e6:00:00/40 tag 1 ncq 524288 out
      [2649467.444859]          res f4/b2:01:01:00:00/00:00:00:10:f4/00 Emask 0x2 (HSM violation)
      [2649467.598529] ata2.01: status: { Busy }
      [2649467.675040] ata2.01: error: { ICRC IDNF }
      [2649467.750713] ata2.01: failed command: WRITE FPDMA QUEUED
      [2649467.826947] ata2.01: cmd 61/00:20:3f:f8:f2/04:00:e6:00:00/40 tag 4 ncq 524288 out
      [2649467.826947]          res f4/b2:01:01:00:00/00:00:00:40:f4/00 Emask 0x2 (HSM violation)
      [2649467.980702] ata2.01: status: { Busy }
      [2649468.057229] ata2.01: error: { ICRC IDNF }
      [2649468.133538] ata2.01: failed command: WRITE FPDMA QUEUED
      [2649468.209410] ata2.01: cmd 61/00:30:3f:fc:f2/04:00:e6:00:00/40 tag 6 ncq 524288 out
      [2649468.209410]          res f4/b2:01:01:00:00/00:00:00:60:f4/00 Emask 0x2 (HSM violation)
      [2649468.363981] ata2.01: status: { Busy }
      [2649468.441165] ata2.01: error: { ICRC IDNF }
      [2649468.517062] ata2.01: failed command: WRITE FPDMA QUEUED
      [2649468.593622] ata2.01: cmd 61/00:40:3f:f4:f2/04:00:e6:00:00/40 tag 8 ncq 524288 out
      [2649468.593622]          res f4/b2:01:01:00:00/00:00:00:80:f4/00 Emask 0x2 (HSM violation)
      [2649468.748634] ata2.01: status: { Busy }
      [2649468.825379] ata2.01: error: { ICRC IDNF }
      [2649468.901975] ata2.01: failed command: WRITE FPDMA QUEUED
      [2649468.978331] ata2.01: cmd 61/00:58:3f:64:f2/04:00:e6:00:00/40 tag 11 ncq 524288 out
      [2649468.978331]          res f4/b2:01:01:00:00/00:00:00:b0:f4/00 Emask 0x2 (HSM violation)
      [2649469.133575] ata2.01: status: { Busy }
      [2649469.210987] ata2.01: error: { ICRC IDNF }
      [2649469.287550] ata2.01: failed command: WRITE FPDMA QUEUED
      [2649469.364029] ata2.01: cmd 61/00:68:3f:f0:f2/04:00:e6:00:00/40 tag 13 ncq 524288 out
      [2649469.364029]          res f4/b2:01:01:00:00/00:00:00:d0:f4/00 Emask 0x2 (HSM violation)
      [2649469.518738] ata2.01: status: { Busy }
      [2649469.596631] ata2.01: error: { ICRC IDNF }
      [2649469.673598] ata2.01: failed command: WRITE FPDMA QUEUED
      [2649469.751000] ata2.01: cmd 61/00:78:3f:6c:f2/04:00:e6:00:00/40 tag 15 ncq 524288 out
      [2649469.751000]          res 61/00:78:3f:6c:f2/00:00:e6:00:00/40 Emask 0x1 (device error)
      [2649469.906991] ata2.01: status: { DRDY DF ERR }


## 2013-08-18

 * Due to [bug][collectd-bug] in version of `collectd` distributed in Raring, forced to install from source.
   Source tree can be found in `~bgamari/repos/collectd`, installed to `/opt/collectd`
 * Configured `goggle` to push samples to `goldnerlab`
 * Also finally fixed [visage][] installation on `goldnerlab`, start script can be found at `~bgamari/start-visage.conf`

## 2013-07-17

 * Installed and configured [collectd][] and [visage][] on goldnerlab and goggle
 * Wrote a small [shim][collectd-cuda] to get CUDA statistics into collectd

[collectd]: http://www.collectd.org/
[visage]: https://collectd.org/wiki/index.php/Visage
[collectd-cuda]: https://github.com/bgamari/cuda-collectd

## 2013-06-07
 
 * Noticed that `sdl` dropped out
   * `dmesg`
        [ 2289.401292] end_request: I/O error, dev sdl, sector 223426111
        [ 2289.401314] sd 7:3:0:0: [sdl]  Result: hostbyte=DID_OK driverbyte=DRIVER_SENSE
        [ 2289.401317] sd 7:3:0:0: [sdl]  Sense Key : Aborted Command [current] [descriptor]
        [ 2289.401320] Descriptor sense data with sense descriptors (in hex):
        [ 2289.401322]         72 0b 00 00 00 00 00 0c 00 0a 80 00 00 00 51 e0 
        [ 2289.401329]         00 00 00 01 
        [ 2289.401333] sd 7:3:0:0: [sdl]  Add. Sense: No additional sense information
        [ 2289.401336] sd 7:3:0:0: [sdl] CDB: Write(10): 2a 00 0d 51 3a 3f 00 04 00 00
        [ 2289.401342] end_request: I/O error, dev sdl, sector 223427135
        [ 2289.401359] ata8: EH complete
        [ 2289.401830] ata8.03: detaching (SCSI 7:3:0:0)

   * Not entiredly sure which drive this is, going to label all drives
 * Looking back at `sdb` again:
   * On for 3.3 years
   * 458 bad sectors

 * Recompiled Amber 11 against CUDA 5.0
 * Upgraded to Ubuntu 12.10 and then 13.04, rebuilt Amber

## 2013-06-06

 * Amber 11 and 12 crashing with a variety of unspecified kernel launch
   failures
 * Moved from CUDA toolkit 4.2 to 5.0
 * Moving from `nvidia` driver version 319.23 to 4.304.54 appears to fix crashes
   in Amber 12
   
## 2013-04-15 

 * `sdb` crashed
   * ST31000528AS (Serial #9VP3YFPT)
 * Removed final data from old `lvm` volumes

## 2011-11-13

 * Power supply failure

## Some time around 2011-09

 * Loss of btrfs data volume due to loose eSATA cable
 * Reformatted volume with more traditional `md` RAID and ext4

## 2011-08-16

 * Upgraded to Ubuntu 11.04

## 2011-01-02

 * Configuration of external 16 TB RAID array

## 2010-02-16

 * Ordered two Seagate Barracuda 7200.12 1TB drivers and new 700W PSU
 * Installed internally

## 2009-12

 * Initial configuration running Ubuntu under VM and other craziness
 * Does anyone remember when exactly this was?

